import numpy as np
import pygame as pg
import matplotlib.pyplot as plt
import random
import os
import time
import csv
import datetime
import configparser
import json
from logging import getLogger, config
from abc import ABCMeta, abstractmethod

# ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from agents import FrierenRLAgent, FernRLAgent, StarkRLAgent
from env import EnhancedFrierenAdventureEnv
from ui import Grid, Graph
from utils import Func
from utils.experiment import compare_learning_effect

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æŒã¤ã¹ãå±æ€§ã®æŠ½è±¡ã‚¯ãƒ©ã‚¹
class Attribute(metaclass=ABCMeta):
    @abstractmethod
    def get_name(self): pass

    @abstractmethod
    def get_no(self): pass

    @abstractmethod
    def coord(self): pass

# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å–ã‚Šå¾—ã‚‹è¡Œå‹•ã®æŠ½è±¡ã‚¯ãƒ©ã‚¹
class Behavior(metaclass=ABCMeta):
    @abstractmethod
    def move(self): pass

    @abstractmethod
    def reset(self): pass

# ç’°å¢ƒå®šç¾©ã®ã‚¯ãƒ©ã‚¹
class Environment:
    # ä¿®æ­£: ãƒã‚¸ãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼ã‚’å®šæ•°ã¨ã—ã¦å®šç¾©
    GRID = np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
                     [1, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 1],
                     [1, 0, 2, 2, 0, 1, 0, 0, 0, 0, 0, 1],
                     [1, 0, 0, 2, 0, 2, 2, 0, 0, 2, 2, 1],
                     [1, 0, 0, 0, 0, 2, 2, 0, 2, 0, 0, 1],
                     [1, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 1],
                     [1, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 1],
                     [1, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 1],
                     [1, 0, 0, 2, 2, 2, 2, 0, 0, 0, 0, 1],
                     [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])

class Agent(Attribute, Behavior):
    def __init__(self, name: str, no: int, x: int, y: int):
        self.__name = name
        self.__no = no
        self.__x = x
        self.__y = y

    def get_name(self) -> str:
        return self.__name

    def get_no(self) -> int:
        return self.__no

    @property
    def coord(self):
        return self.__x, self.__y

    @coord.setter
    def coord(self, xy: list):
        self.__x = xy[0]
        self.__y = xy[1]

    def collision(self, diff_y: int, diff_x: int) -> int:
        if Environment.GRID[self.__y + diff_y][self.__x + diff_x] == Grid.ROAD:
            return True
        else:
            return False

    def move(self, vertical: int, horizontal: int):
        if self.collision(vertical, horizontal):
            self.__y += vertical
            self.__x += horizontal

    def action(self, num: int):
        if num == 0:  # ä¸Š
            self.move(-1, 0)
        elif num == 1:  # ä¸‹
            self.move(1, 0)
        elif num == 2:  # å·¦
            self.move(0, -1)
        elif num == 3:  # å³
            self.move(0, 1)

    def strategy(self):
        pass

    def reset(self):
        pass

class Friend(Agent):
    def __init__(self, name: str, no: int, x: int, y: int):
        super().__init__(name, no, x, y)
        self.__init_x = x
        self.__init_y = y

    def action(self, num: int):
        # ä¿®æ­£: å®šæ•°ã‚’ä½¿ç”¨
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.ROAD
        if num == 0:  # ä¸Š
            self.move(-1, 0)
        elif num == 1:  # ä¸‹
            self.move(1, 0)
        elif num == 2:  # å·¦
            self.move(0, -1)
        elif num == 3:  # å³
            self.move(0, 1)
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.HUMAN

    def strategy(self, coord: list):
        if (3 > coord[0] - self.__init_x > 0) and (3 > coord[1] - self.__init_y > 0):
            return 1
        elif (-3 < coord[0] - self.__init_x < 0) and (3 > coord[1] - self.__init_y > 0):
            return 3
        elif (-3 < coord[0] - self.__init_x < 0) and (-3 < coord[1] - self.__init_y < 0):
            return 0
        elif(3 > coord[0] - self.__init_x > 0) and (-3 < coord[1] - self.__init_y < 0):
            return 2
        else:
            return random.randint(0, 3)

    def reset(self):
        # ä¿®æ­£: å®šæ•°ã‚’ä½¿ç”¨
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.ROAD
        self.coord = [self.__init_x, self.__init_y]
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.HUMAN

class Enemy(Agent):
    def __init__(self, name: str, no: int, x: int, y: int):
        super().__init__(name, no, x, y)
        self.__init_x = x
        self.__init_y = y

    def action(self, num: int):
        # ä¿®æ­£: å®šæ•°ã‚’ä½¿ç”¨
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.ROAD
        if num == 0:  # å·¦ä¸Š
            self.move(-1, -1)
        elif num == 1:  # å³ä¸‹
            self.move(1, 1)
        elif num == 2:  # å·¦ä¸‹
            self.move(1, -1)
        elif num == 3:  # å³ä¸Š
            self.move(-1, 1)
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.ALIEN

    def reset(self):
        # ä¿®æ­£: å®šæ•°ã‚’ä½¿ç”¨
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.ROAD
        self.coord = [self.__init_x, self.__init_y]
        Environment.GRID[self.coord[1]][self.coord[0]] = Grid.ALIEN

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰"""
    # å¤–éƒ¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
    conf = configparser.ConfigParser()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®å‡¦ç†
    if not os.path.exists('./conf/conf.ini'):
        print("è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ./conf/conf.ini")
        print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œã—ã¾ã™...")
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’è¨­å®š
        conf.add_section('ENVIRONMENT')
        conf.set('ENVIRONMENT', 'AGT1_COORDX', '1')
        conf.set('ENVIRONMENT', 'AGT1_COORDY', '1')
        conf.set('ENVIRONMENT', 'AGT2_COORDX', '2')
        conf.set('ENVIRONMENT', 'AGT2_COORDY', '1')
        conf.set('ENVIRONMENT', 'AGT3_COORDX', '3')
        conf.set('ENVIRONMENT', 'AGT3_COORDY', '1')
        conf.set('ENVIRONMENT', 'ENEMY_COORDX', '10')
        conf.set('ENVIRONMENT', 'ENEMY_COORDY', '10')
        
        conf.add_section('SYSTEM')
        conf.set('SYSTEM', 'MAX_STEP', '50')
        conf.set('SYSTEM', 'MAX_EPISODE', '10')
    else:
        conf.read('./conf/conf.ini')

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã®ãŸã‚ã®è¨­å®š
    if not os.path.exists('./conf/log_config.json'):
        print("ãƒ­ã‚°è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: ./conf/log_config.json")
        print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ­ã‚°è¨­å®šã§å®Ÿè¡Œã—ã¾ã™...")
        log_conf = {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "standard": {
                    "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
                }
            },
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "formatter": "standard",
                    "level": "INFO",
                    "stream": "ext://sys.stdout"
                }
            },
            "root": {
                "handlers": ["console"],
                "level": "INFO"
            }
        }
    else:
        with open('./conf/log_config.json', 'r') as f:
            log_conf = json.load(f)
    
    # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
    log_dir = 'log'
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
        print(f"ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã—ãŸ: {log_dir}")
    
    config.dictConfig(log_conf)
    logger = getLogger(__name__)

    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”Ÿæˆ
    human = []
    alien = []
    
    # ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³ï¼ˆmageï¼‰
    tmp_x = int(conf.get('ENVIRONMENT', 'AGT1_COORDX', fallback='1'))
    tmp_y = int(conf.get('ENVIRONMENT', 'AGT1_COORDY', fallback='1'))
    human.append(Friend('ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³', 1, tmp_x, tmp_y))
    Environment.GRID[tmp_y][tmp_x] = Grid.HUMAN
    
    # ãƒ•ã‚§ãƒ«ãƒ³ï¼ˆhealerï¼‰
    tmp_x = int(conf.get('ENVIRONMENT', 'AGT2_COORDX', fallback='2'))
    tmp_y = int(conf.get('ENVIRONMENT', 'AGT2_COORDY', fallback='1'))
    human.append(Friend('ãƒ•ã‚§ãƒ«ãƒ³', 2, tmp_x, tmp_y))
    Environment.GRID[tmp_y][tmp_x] = Grid.HUMAN
    
    # ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯ï¼ˆwarriorï¼‰
    tmp_x = int(conf.get('ENVIRONMENT', 'AGT3_COORDX', fallback='3'))
    tmp_y = int(conf.get('ENVIRONMENT', 'AGT3_COORDY', fallback='1'))
    human.append(Friend('ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯', 3, tmp_x, tmp_y))
    Environment.GRID[tmp_y][tmp_x] = Grid.HUMAN
    
    # ã‚¨ã‚¤ãƒªã‚¢ãƒ³
    tmp_x = int(conf.get('ENVIRONMENT', 'ENEMY_COORDX', fallback='10'))
    tmp_y = int(conf.get('ENVIRONMENT', 'ENEMY_COORDY', fallback='10'))
    alien.append(Enemy('ã‚¨ã‚¤ãƒªã‚¢ãƒ³', 1, tmp_x, tmp_y))
    Environment.GRID[tmp_y][tmp_x] = Grid.ALIEN
    
    num_human = 3
    num_alien = 1
    contact = [0, 0, 0]
    history = [[] for _ in range(num_human)]
    
    # å®Ÿè¡Œã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹å¤‰æ•°
    step = 0
    episode = 0
    max_step = int(conf.get('SYSTEM', 'MAX_STEP', fallback='50'))
    max_episode = int(conf.get('SYSTEM', 'MAX_EPISODE', fallback='10'))

    # ã‚°ãƒªãƒƒãƒ‰ã®ç”Ÿæˆ
    grid = Grid(Environment.GRID.shape)

    # ã‚°ãƒ©ãƒ•ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ
    graph = Graph('Simulation result', 'Number of episodes', 'Number of contacts', num_human)

    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ç”¨
    func = Func()

    # å®Ÿè¡Œã™ã‚‹ã‹å¦ã‹ã‚’å•ã†é–‹å§‹ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ãƒœãƒƒã‚¯ã‚¹
    if input('ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ (y/n): ') == 'y':
        logger.info('ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé–‹å§‹ã•ã‚Œã¾ã—ãŸ')
        
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        while episode < max_episode:
            logger.info('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰%dã‚’å®Ÿè¡Œã—ã¾ã™', episode)
            
            while step < max_step:
                # ã‚°ãƒªãƒƒãƒ‰ã®è¡¨ç¤ºå†…å®¹è¨­å®š
                grid.reflect(Environment.GRID, human, alien)
                # ã‚°ãƒªãƒƒãƒ‰è¡¨ç¤ºã®æ›´æ–°
                pg.display.update()
                
                # Do the action
                for i in range(0, num_human, 1):
                    human[i].action(human[i].strategy(alien[0].coord))
                for j in range(0, num_alien, 1):
                    alien[j].action(random.randint(0, 3))
                
                # humanã¨alienã§å½“ãŸã‚Šåˆ¤å®š
                for i in range(0, num_human, 1):
                    for j in range(0, num_alien, 1):
                        if (abs(human[i].coord[0] - alien[j].coord[0])) <= 1 and \
                                (abs(human[i].coord[1] - alien[j].coord[1])) <= 1:
                            contact[i] += 1
                            break
                
                step += 1
                time.sleep(0.01)

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®çµ‚äº†å‡¦ç†
            for i in range(0, num_human, 1):
                history[i].append(contact[i])
            
            # åˆæœŸåŒ–
            step = 0
            for i in range(0, num_human, 1):
                contact[i] = 0
            
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã®ã‚«ã‚¦ãƒ³ãƒˆã‚¢ãƒƒãƒ—
            episode += 1
            
            # å…¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒªã‚¹ãƒãƒ¼ãƒ³
            for i in range(0, num_human, 1):
                human[i].reset()
            for j in range(0, num_alien, 1):
                alien[j].reset()
            
            graph.data(history)
            plt.pause(.001)

        # è¡çªå±¥æ­´ã®historyã‚’ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡ºåŠ›
        func.save(np.array(history))
        # ã‚°ãƒ©ãƒ•ã®è‡ªå‹•ä¿å­˜
        graph.save(func.get_log_date() + '_graph.png')
        
        print('ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒçµ‚äº†ã—ã¾ã—ãŸï¼')
        logger.info('ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã—ãŸ')
        
        # pygameã®çµ‚äº†å‡¦ç†
        pg.quit()
        # ã‚°ãƒ©ãƒ•ã‚’ç”»é¢ã«è¡¨ç¤º
        graph.show()
        graph.close()
        
        # ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import atexit
        atexit.register(pg.quit)

def moving_average(data, window_size=20):
    import numpy as np
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

def run_frieren_training():
    """ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³å†’é™ºã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å­¦ç¿’å®Ÿè¡Œ"""
    print("=== ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³å†’é™ºãƒ‘ãƒ¼ãƒ†ã‚£å¼·åŒ–å­¦ç¿’é–‹å§‹ ===")
    
    # ç’°å¢ƒã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–
    env = EnhancedFrierenAdventureEnv()
    agents = [
        FrierenRLAgent(state_size=16, action_size=4),
        FernRLAgent(state_size=16, action_size=4),
        StarkRLAgent(state_size=16, action_size=4)
    ]
    
    # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç’°å¢ƒã«è¨­å®š
    env.set_agents(agents)
    
    # å­¦ç¿’å®Ÿè¡Œ
    scores, win_rates = env.train_agents(episodes=1000, batch_size=64)
    
    # çµæœå¯è¦–åŒ–
    import numpy as np
    plt.figure(figsize=(12, 4))
    plt.subplot(1, 2, 1)
    plt.plot(scores, alpha=0.3, label='Raw')
    plt.plot(moving_average(scores), color='red', label='Moving Avg')
    plt.title('Training Scores')
    plt.xlabel('Episode')
    plt.ylabel('Total Reward')
    plt.legend()
    
    plt.subplot(1, 2, 2)
    episodes = range(0, len(win_rates) * 100, 100)
    plt.plot(episodes, win_rates, alpha=0.3, label='Raw')
    if len(win_rates) > 5:
        plt.plot(episodes[len(episodes)-len(moving_average(win_rates, 3)):], moving_average(win_rates, 3), color='red', label='Moving Avg')
    plt.title('Win Rate')
    plt.xlabel('Episode')
    plt.ylabel('Win Rate')
    plt.legend()
    plt.tight_layout()
    plt.show()
    
    print("\n=== å­¦ç¿’å¾Œã®ãƒ†ã‚¹ãƒˆæˆ¦é—˜ ===")
    test_battle(env)
    
    # --- è¿½åŠ : çŠ¶æ³ã”ã¨ã®è¡Œå‹•é¸æŠå‰²åˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— ---
    from ui.plot import Graph
    print("\n=== çŠ¶æ³ã”ã¨ã®è¡Œå‹•é¸æŠå‰²åˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ— ===")
    graph = Graph('Action Heatmap', '', '', 1)
    # å…¨ã‚­ãƒ£ãƒ©ã¾ã¨ã‚ã¦
    graph.plot_action_heatmap(env.action_log, char_name=None, show=True)
    # ã‚­ãƒ£ãƒ©ã”ã¨
    for char in ['ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³', 'ãƒ•ã‚§ãƒ«ãƒ³', 'ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯']:
        graph.plot_action_heatmap(env.action_log, char_name=char, show=True)
    # ---
    
    # ä¿®æ­£: æ¯”è¼ƒå®Ÿé¨“ã‚’è¿½åŠ 
    print("\n=== å­¦ç¿’åŠ¹æœã®æ¯”è¼ƒå®Ÿé¨“ ===")
    compare_learning_effect(env, agents, num_battles=50)

def test_battle(env):
    """å­¦ç¿’å¾Œã®ãƒ†ã‚¹ãƒˆæˆ¦é—˜"""
    # æ¢ç´¢ç‡ã‚’0ã«ã—ã¦ãƒ†ã‚¹ãƒˆ
    for agent in env.agents:
        if hasattr(agent, 'epsilon'):
            agent.epsilon = 0
    
    state = env.reset()
    done = False
    turn = 0
    
    print(f"æˆ¦é—˜é–‹å§‹ï¼")
    print(f"ãƒ‘ãƒ¼ãƒ†ã‚£: ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³(HP:{env.party[0].hp}), ãƒ•ã‚§ãƒ«ãƒ³(HP:{env.party[1].hp}), ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯(HP:{env.party[2].hp})")
    print(f"ãƒœã‚¹: {env.boss.name}(HP:{env.boss.hp})")
    
    while not done and turn < 50:
        turn += 1
        next_state, rewards, done, info = env.step()
        print(f"\nã‚¿ãƒ¼ãƒ³ {turn}:")
        print(f"ãƒ‘ãƒ¼ãƒ†ã‚£: ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³(HP:{env.party[0].hp}), ãƒ•ã‚§ãƒ«ãƒ³(HP:{env.party[1].hp}), ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯(HP:{env.party[2].hp})")
        print(f"ãƒœã‚¹: {env.boss.name}(HP:{env.boss.hp})")
        state = next_state
    
    if not env.boss.alive:
        print(f"\nğŸ‰ å‹åˆ©ï¼ {turn}ã‚¿ãƒ¼ãƒ³ã§ãƒœã‚¹ã‚’æ’ƒç ´ã—ã¾ã—ãŸï¼")
    elif all(not char.alive for char in env.party):
        print(f"\nğŸ’€ æ•—åŒ—... ãƒ‘ãƒ¼ãƒ†ã‚£ãŒå…¨æ»…ã—ã¾ã—ãŸ...")
    else:
        print(f"\nâ° æ™‚é–“åˆ‡ã‚Œ...")

def test_manhattan_distance_system():
    """ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    print("=== ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ ===")
    
    # ç’°å¢ƒã‚’åˆæœŸåŒ–
    env = EnhancedFrierenAdventureEnv()
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆ
    class DummyAgent:
        def get_action(self, state, *args):
            return random.randint(0, 3)  # ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•
    
    # ãƒ€ãƒŸãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨­å®š
    dummy_agents = [DummyAgent() for _ in range(3)]
    env.set_agents(dummy_agents)
    
    # ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã®ãƒ†ã‚¹ãƒˆ
    env.test_manhattan_distance()
    
    # ç°¡å˜ãªæˆ¦é—˜ãƒ†ã‚¹ãƒˆ
    print("\n=== æˆ¦é—˜ãƒ†ã‚¹ãƒˆ ===")
    state = env.reset()
    done = False
    turn = 0
    
    print(f"åˆæœŸçŠ¶æ…‹:")
    print(f"ãƒ‘ãƒ¼ãƒ†ã‚£: ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³(HP:{env.party[0].hp}), ãƒ•ã‚§ãƒ«ãƒ³(HP:{env.party[1].hp}), ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯(HP:{env.party[2].hp})")
    print(f"ãƒœã‚¹: {env.boss.name}(HP:{env.boss.hp})")
    print(f"ä½ç½®æƒ…å ±: {env.character_positions}")
    
    # 5ã‚¿ãƒ¼ãƒ³ã®æˆ¦é—˜ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    while not done and turn < 5:
        turn += 1
        next_state, rewards, done, info = env.step()
        
        print(f"\nã‚¿ãƒ¼ãƒ³ {turn}:")
        print(f"ãƒ‘ãƒ¼ãƒ†ã‚£: ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³(HP:{env.party[0].hp}), ãƒ•ã‚§ãƒ«ãƒ³(HP:{env.party[1].hp}), ã‚·ãƒ¥ã‚¿ãƒ«ã‚¯(HP:{env.party[2].hp})")
        print(f"ãƒœã‚¹: {env.boss.name}(HP:{env.boss.hp})")
        print(f"ä½ç½®æƒ…å ±: {env.character_positions}")
        
        # ãƒãƒˆãƒ«ãƒ­ã‚°ã‚’è¡¨ç¤º
        if info.get('battle_log'):
            print("ãƒãƒˆãƒ«ãƒ­ã‚°:")
            for log in info['battle_log'][-3:]:  # æœ€æ–°3ä»¶
                print(f"  - {log}")
        
        state = next_state
    
    print(f"\næˆ¦é—˜çµ‚äº†: {'å‹åˆ©' if not env.boss.alive else 'æ•—åŒ—' if all(not char.alive for char in env.party) else 'ç¶™ç¶š'}")

if __name__ == "__main__":
    print("=== ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³å”åŠ›å‹ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰å¼·åŒ–å­¦ç¿’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ ===")
    print("å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¦ãã ã•ã„:")
    print("1. ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã‚·ã‚¹ãƒ†ãƒ ãƒ†ã‚¹ãƒˆ")
    print("2. ã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("3. ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³å¼·åŒ–å­¦ç¿’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
    print("4. å…¨ã¦å®Ÿè¡Œ")
    
    try:
        choice = input("é¸æŠã—ã¦ãã ã•ã„ (1-4): ").strip()
        
        if choice == "1":
            test_manhattan_distance_system()
        elif choice == "2":
            main()
        elif choice == "3":
            run_frieren_training()
        elif choice == "4":
            # ãƒãƒ³ãƒãƒƒã‚¿ãƒ³è·é›¢ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆã‚’è¿½åŠ 
            test_manhattan_distance_system()
            
            # ã¾ãšã‚°ãƒªãƒƒãƒ‰ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
            main()
            # ç¶šã‘ã¦ãƒ•ãƒªãƒ¼ãƒ¬ãƒ³å¼·åŒ–å­¦ç¿’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
            run_frieren_training()
        else:
            print("ç„¡åŠ¹ãªé¸æŠã§ã™ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨ã¦å®Ÿè¡Œã—ã¾ã™ã€‚")
            test_manhattan_distance_system()
            main()
            run_frieren_training()
            
    except KeyboardInterrupt:
        print("\nãƒ—ãƒ­ã‚°ãƒ©ãƒ ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸã€‚")
    except Exception as e:
        print(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¨ã¦å®Ÿè¡Œã—ã¾ã™ã€‚")
        test_manhattan_distance_system()
        main()
        run_frieren_training() 